{"cells":[{"cell_type":"markdown","metadata":{"id":"Ft8lY0EPGv58"},"source":["## Import Needed Modules"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: nltk in c:\\users\\fin365\\appdata\\roaming\\python\\python39\\site-packages (3.8.1)\n","Requirement already satisfied: click in c:\\users\\fin365\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in c:\\program files\\orange\\lib\\site-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in c:\\users\\fin365\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in c:\\users\\fin365\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (4.66.1)\n","Requirement already satisfied: colorama in c:\\program files\\orange\\lib\\site-packages (from click->nltk) (0.4.6)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# %pip install tensorflow\n","%pip install nltk\n","# %pip install --upgrade pip"]},{"cell_type":"code","execution_count":2,"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"execution":{"iopub.execute_input":"2023-12-13T02:54:52.995089Z","iopub.status.busy":"2023-12-13T02:54:52.993952Z","iopub.status.idle":"2023-12-13T02:54:53.001861Z","shell.execute_reply":"2023-12-13T02:54:53.000796Z","shell.execute_reply.started":"2023-12-13T02:54:52.995051Z"},"id":"sATsPkyZfGDb","jupyter":{"outputs_hidden":true},"outputId":"375aba4f-8566-4cb4-d256-add054ff0000","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\Fin365\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\Fin365\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import json\n","import random\n","import numpy as np\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From C:\\Users\\Fin365\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]}],"source":["from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout\n","from keras.optimizers import SGD"]},{"cell_type":"markdown","metadata":{"id":"N3Q24tO4Gv6Y"},"source":["## Data Analysis and Preprcessing"]},{"cell_type":"markdown","metadata":{"id":"cSyfLcbzGv6a"},"source":["#### Read data"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-12-13T02:52:14.515889Z","iopub.status.busy":"2023-12-13T02:52:14.515178Z","iopub.status.idle":"2023-12-13T02:52:14.535310Z","shell.execute_reply":"2023-12-13T02:52:14.534425Z","shell.execute_reply.started":"2023-12-13T02:52:14.515850Z"},"id":"8deOQPh2Gv6b","trusted":true},"outputs":[],"source":["data_file = open('intents.json').read()\n","intents = json.loads(data_file)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-12-13T02:52:24.454983Z","iopub.status.busy":"2023-12-13T02:52:24.454186Z","iopub.status.idle":"2023-12-13T02:52:24.459669Z","shell.execute_reply":"2023-12-13T02:52:24.458509Z","shell.execute_reply.started":"2023-12-13T02:52:24.454950Z"},"id":"o90g9bDHfPLc","trusted":true},"outputs":[],"source":["words=[]\n","classes = []\n","documents = []\n","ignore_words = ['?', '!']"]},{"cell_type":"markdown","metadata":{"id":"6k8MD468Gv6e"},"source":["### Preprocessing Text"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"execution":{"iopub.execute_input":"2023-12-13T02:55:00.060484Z","iopub.status.busy":"2023-12-13T02:55:00.060108Z","iopub.status.idle":"2023-12-13T02:55:00.264709Z","shell.execute_reply":"2023-12-13T02:55:00.263030Z","shell.execute_reply.started":"2023-12-13T02:55:00.060454Z"},"id":"uO1HCYhafVSn","jupyter":{"outputs_hidden":true},"outputId":"76123f23-b136-4e68-ae3c-1b79407f33e0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["161 documents\n","107 unique lemmatized words\n","51 classes ['Adventure stories', 'American fiction', 'Architecture', 'Art', 'Biography & Autobiography', 'Body, Mind & Spirit', 'Business & Economics', \"Children's stories\", 'Comics & Graphic Novels', 'Computers', 'Cooking', 'Detective and mystery stories', 'Drama', 'Education', 'English fiction', 'Family & Relationships', 'Fantasy fiction', 'Fiction', 'Foreign Language Study', 'Games', 'Health & Fitness', 'History', 'Humor', 'Juvenile Fiction', 'Juvenile Nonfiction', 'Language Arts & Disciplines', 'Law', 'Literary Collections', 'Literary Criticism', 'Medical', 'Music', 'Nature', 'Performing Arts', 'Philosophy', 'Photography', 'Poetry', 'Political Science', 'Psychology', 'Religion', 'Science', 'Science fiction', 'Self-Help', 'Social Science', 'Sports & Recreation', 'Travel', 'True Crime', 'Young Adult Fiction', 'book_search', 'goodbye', 'greeting', 'thanks']\n"]}],"source":["for intent in intents['intents']:\n","    for pattern in intent['patterns']:\n","\n","        # take each word and tokenize it\n","        w = nltk.word_tokenize(pattern)\n","        words.extend(w)\n","\n","        # adding documents\n","        documents.append((w, intent['tag']))\n","\n","        # adding classes to our class list\n","        if intent['tag'] not in classes:\n","            classes.append(intent['tag'])\n","\n","words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n","words = sorted(list(set(words)))\n","\n","classes = sorted(list(set(classes)))\n","\n","print (len(documents), \"documents\")\n","print (len(words), \"unique lemmatized words\")\n","print (len(classes), \"classes\", classes)"]},{"cell_type":"markdown","metadata":{"id":"4k3QulZYGv6i"},"source":["### Initializing Training Data"]},{"cell_type":"code","execution_count":7,"metadata":{"_kg_hide-output":true,"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"1tjRLRkmfdD3","jupyter":{"outputs_hidden":true},"outputId":"25765ab5-79b5-4028-8073-a2978b4c7845"},"outputs":[{"ename":"ValueError","evalue":"setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (161, 2) + inhomogeneous part.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# shuffle our features and turn into np.array\u001b[39;00m\n\u001b[0;32m     25\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(training)\n\u001b[1;32m---> 26\u001b[0m training \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# create train and test lists. X - patterns, Y - intents\u001b[39;00m\n\u001b[0;32m     29\u001b[0m train_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(training[:, \u001b[38;5;241m0\u001b[39m])\n","\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (161, 2) + inhomogeneous part."]}],"source":["training = []\n","output_empty = [0] * len(classes)\n","for doc in documents:\n","\n","    # initializing bag of words\n","    bag = []\n","\n","    # list of tokenized words for the pattern\n","    pattern_words = doc[0]\n","\n","    # lemmatize each word - create base word, in attempt to represent related words\n","    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n","\n","    # create our bag of words array with 1, if word match found in current pattern\n","    for w in words:\n","        bag.append(1) if w in pattern_words else bag.append(0)\n","\n","    # output is a '0' for each tag and '1' for current tag (for each pattern)\n","    output_row = list(output_empty)\n","    output_row[classes.index(doc[1])] = 1\n","\n","    training.append([bag, output_row])\n","\n","# shuffle our features and turn into np.array\n","random.shuffle(training)\n","training = np.array(training)\n","\n","# create train and test lists. X - patterns, Y - intents\n","train_x = list(training[:, 0])\n","train_y = list(training[:, 1])"]},{"cell_type":"markdown","metadata":{"id":"cjX17dBSGv6m"},"source":["## Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vqhMrg_fff_l","outputId":"c8dfc0cb-6376-42e2-fd3f-65e131bba7e1"},"outputs":[],"source":["# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n","# equal to number of intents to predict output intent with softmax\n","model = Sequential()\n","model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(len(train_y[0]), activation='softmax'))\n","\n","# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n","sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n","model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"wglWiW6Xfkke","jupyter":{"outputs_hidden":true},"outputId":"5b087166-059a-4d3e-92d8-4f5cb4ebeb5d"},"outputs":[],"source":["#fitting and saving the model\n","hist = model.fit(np.array(train_x), np.array(train_y), epochs=500, batch_size=5, verbose=1)\n","model.save('chatbot_model.h5', hist)\n","\n","print(\"model created\")"]},{"cell_type":"markdown","metadata":{"id":"RAxTDBvvHtQQ"},"source":["## Chatbot Prediction"]},{"cell_type":"markdown","metadata":{"id":"HA3t0OHnHzLP"},"source":["### Function to clean user input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUfWyTKvfqN4"},"outputs":[],"source":["def clean_up_sentence(sentence):\n","    sentence_words = nltk.word_tokenize(sentence)\n","    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n","\n","    return sentence_words"]},{"cell_type":"markdown","metadata":{"id":"zI4vJmuIH_B6"},"source":["### Function for Bag of Wrds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rm9IEuyigbWO"},"outputs":[],"source":["def bow(sentence, words, show_details=True):\n","    # tokenize the pattern\n","    sentence_words = clean_up_sentence(sentence)\n","\n","    # bag of words - matrix of N words, vocabulary matrix\n","    bag = [0] * len(words)\n","    for s in sentence_words:\n","        for i, w in enumerate(words):\n","            if w == s:\n","\n","                # assign 1 if current word is in the vocabulary position\n","                bag[i] = 1\n","                if show_details:\n","                    print (\"found in bag: %s\" % w)\n","\n","    return(np.array(bag))"]},{"cell_type":"markdown","metadata":{"id":"Kf81e3EhISpJ"},"source":["### Function for Class Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X0HjJnAGgeed"},"outputs":[],"source":["def predict_class(sentence, model):\n","    # filter out predictions below a threshold\n","    p = bow(sentence, words,show_details=False)\n","    res = model.predict(np.array([p]))[0]\n","    ERROR_THRESHOLD = 0.25\n","    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n","\n","    # sort by strength of probability\n","    results.sort(key=lambda x: x[1], reverse=True)\n","    return_list = []\n","    for r in results:\n","        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n","\n","    return return_list"]},{"cell_type":"markdown","metadata":{"id":"GuJ1wP-HIeZK"},"source":["### Function to get chatbot response"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8ZzmKsegnZH"},"outputs":[],"source":["def getResponse(ints, intents_json):\n","    tag = ints[0]['intent']\n","    list_of_intents = intents_json['intents']\n","    for i in list_of_intents:\n","        if(i['tag']== tag):\n","            result = random.choice(i['responses'])\n","            break\n","\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"HWLoe9_1Int8"},"source":["### Chatbot Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BMsqwkoXgq7B"},"outputs":[],"source":["def chatbot_response(msg):\n","    ints = predict_class(msg, model)\n","    res = getResponse(ints, intents)\n","    return res"]},{"cell_type":"markdown","metadata":{"id":"4B6D9ImlI8LS"},"source":["## Chatbot"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gRbMamvgg3_i","outputId":"8d5c7129-fc12-4e3e-b2c5-0cec3948e5a0"},"outputs":[],"source":["chatbot_response('Recommend a book in History')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4155764,"sourceId":7188058,"sourceType":"datasetVersion"}],"dockerImageVersionId":30616,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
